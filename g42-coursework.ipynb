{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Neural Computation Coursework<br>Group 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3><b>Latif Rozaqi (1980172)<br>Latif Rozaqi (1980172)<br>Latif Rozaqi (1980172)<br>Latif Rozaqi (1980172)<br>Latif Rozaqi (1980172)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    <b>I. Data</b>\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.1 Import Libraries and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### Libraries #####################################\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import h5py, os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.utils as vutils\n",
    "from functions import transforms as T\n",
    "from functions.subsample import MaskFunc\n",
    "from scipy.io import loadmat\n",
    "from matplotlib import pyplot as plt\n",
    "import csv\n",
    "import time\n",
    "from skimage.measure import compare_ssim \n",
    "\n",
    "################################## Functions #######################################\n",
    "def show_slices(data, slice_nums, cmap=None): # visualisation\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    for i, num in enumerate(slice_nums):\n",
    "        plt.subplot(1, len(slice_nums), i + 1)\n",
    "        plt.imshow(data[num], cmap=cmap)\n",
    "        plt.axis('off')\n",
    "\n",
    "class MRIDataset(DataLoader):\n",
    "    def __init__(self, data_list, acceleration, center_fraction, use_seed, cplx):\n",
    "        self.data_list = data_list\n",
    "        self.acceleration = acceleration\n",
    "        self.center_fraction = center_fraction\n",
    "        self.use_seed = use_seed\n",
    "        self.cplx=cplx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subject_id = self.data_list[idx]\n",
    "        return get_epoch_batch(subject_id, self.acceleration, self.center_fraction, self.use_seed, self.cplx)\n",
    "\n",
    "def get_epoch_batch(subject_id, acc, center_fract, use_seed=True, cplx=False):\n",
    "    ''' random select a few slices (batch_size) from each volume'''\n",
    "\n",
    "    fname, rawdata_name, slice = subject_id  \n",
    "    Cplx=cplx\n",
    "    with h5py.File(rawdata_name, 'r') as data:\n",
    "        rawdata = data['kspace'][slice]\n",
    "                      \n",
    "    rawdata=crop_img(rawdata,320,320)\n",
    "            \n",
    "    slice_kspace = T.to_tensor(rawdata).unsqueeze(0)\n",
    "    S, Ny, Nx, ps = slice_kspace.shape\n",
    "    \n",
    "    slice_kspace_abs=torch.log(T.complex_abs(slice_kspace) + 1e-9)\n",
    "    # apply random mask\n",
    "    shape = np.array(slice_kspace.shape)\n",
    "    shape2 = np.array(slice_kspace2.shape)\n",
    "    mask_func = MaskFunc(center_fractions=[center_fract], accelerations=[acc])\n",
    "    seed = None if not use_seed else tuple(map(ord, fname))\n",
    "    mask = mask_func(shape, seed)\n",
    "    mask2 = mask_func(shape2, seed)\n",
    "      \n",
    "    # undersample\n",
    "    masked_kspace = torch.where(mask == 0, torch.Tensor([0]), slice_kspace)\n",
    "    masked_kspace2 = torch.where(mask2 == 0, torch.Tensor([0]), slice_kspace2)\n",
    "    masks = mask.repeat(S, Ny, 1, ps)\n",
    "    masked_kspace_abs=torch.log(T.complex_abs(masked_kspace) + 1e-9)\n",
    "\n",
    "    img_gt, img_und = T.ifft2(slice_kspace), T.ifft2(masked_kspace)\n",
    "    # perform data normalization which is important for network to learn useful features\n",
    "    # during inference there is no ground truth image so use the zero-filled recon to normalize\n",
    "    norm = T.complex_abs(img_und).max()\n",
    "    if norm < 1e-6: norm = 1e-6\n",
    "    \n",
    "    # normalized data\n",
    "    img_gt, img_und, rawdata_und = img_gt/norm, img_und/norm, masked_kspace/norm\n",
    "       \n",
    "    img_gt=T.center_crop(T.complex_abs(img_gt),[320,320])\n",
    "    img_und=T.center_crop(T.complex_abs(img_und),[320,320])\n",
    "    if not Cplx:\n",
    "        return img_und, img_gt # x is the undersampled image y is the ground truth\n",
    "    else:\n",
    "        return masked_kspace2, slice_kspace2\n",
    "\n",
    "def load_data_path(train_data_path, val_data_path):\n",
    "    \"\"\" Go through each subset (training, validation) and list all \n",
    "    the file names, the file paths and the slices of subjects in the training and validation sets \n",
    "    \"\"\"\n",
    "\n",
    "    data_list = {}\n",
    "    train_and_val = ['train', 'val']\n",
    "    data_path = [train_data_path, val_data_path]\n",
    "      \n",
    "    for i in range(len(data_path)):\n",
    "\n",
    "        data_list[train_and_val[i]] = []\n",
    "        \n",
    "        which_data_path = data_path[i]\n",
    "    \n",
    "        for fname in sorted(os.listdir(which_data_path)):\n",
    "            \n",
    "            subject_data_path = os.path.join(which_data_path, fname)\n",
    "                     \n",
    "            if not os.path.isfile(subject_data_path): continue \n",
    "            \n",
    "            with h5py.File(subject_data_path, 'r') as data:\n",
    "                num_slice = data['kspace'].shape[0]\n",
    "                \n",
    "            # the first 5 slices are mostly noise so it is better to exlude them\n",
    "            data_list[train_and_val[i]] += [(fname, subject_data_path, slice) for slice in range(5, num_slice)]\n",
    "    \n",
    "    return data_list    \n",
    "\n",
    "def show_img(data, slice_nums, train_num, cmap=None): # visualisation\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    for i, num in enumerate(slice_nums):\n",
    "        plt.subplot(1, len(slice_nums), i + 1)\n",
    "        plt.imshow(data[train_num,num], cmap=cmap)\n",
    "        plt.axis('off')\n",
    "        \n",
    "def ssim(gt, pred):\n",
    "    \"\"\" Compute Structural Similarity Index Metric (SSIM). \"\"\"\n",
    "    return compare_ssim(\n",
    "        gt.transpose(1, 2, 0), pred.transpose(1, 2, 0), multichannel=True, data_range=gt.max()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.2 Configure the Training Dataset\n",
    "    <h4> 1.2.1 Split Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Here we split the Dataset into training and validation data with the proportion of 80:20 randomly with seed generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random number seed generator\n",
    "SeedGen = 42\n",
    "# set the path of trainig dataset\n",
    "data_path_train = '/data/local/NC2019MRI/train'\n",
    "data_path_val = '/data/local/NC2019MRI/train'\n",
    "# compute the number of files inside the directory\n",
    "num_files=len(os.listdir(data_path_train))\n",
    "index=(np.arange(num_files))\n",
    "# generate random number for index\n",
    "np.random.seed(SeedGen)\n",
    "# shuffle index randomly\n",
    "np.random.shuffle(index)\n",
    "# split the train and validate data by 80:20\n",
    "train_index=np.sort(index[:int(num_files*0.8)])\n",
    "validate_index=np.sort(index[int(num_files*0.8):])\n",
    "# create dictionary contains the information about the traindata and validation data from train dataset\n",
    "data_list = {}\n",
    "data_list['train']=[]\n",
    "data_list['validate']=[]\n",
    "for idx,fname in enumerate(sorted(os.listdir(data_path_train))): \n",
    "    subject_data_path = os.path.join(data_path_train, fname)         \n",
    "    if not os.path.isfile(subject_data_path): continue         \n",
    "    with h5py.File(subject_data_path, 'r') as data:\n",
    "            num_slice = data['kspace'].shape[0]\n",
    "    if len(np.where(train_index==idx)[0]):\n",
    "        data_list['train']+=[(fname,subject_data_path,slice) for slice in range(5,num_slice)]\n",
    "    else:\n",
    "        data_list['validate']+=[(fname,subject_data_path,slice) for slice in range(5,num_slice)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=5 # set training batch size\n",
    "acc = 8 # set the cartesian undersampling acceleration factor\n",
    "cen_fract = 0.04\n",
    "seed = False # random masks for each slice \n",
    "num_workers = 0 # data loading is faster using a bigger number for num_workers. 0 means using one cpu to load data\n",
    "cplx = False # True: choose real-valued image otherwise: complex k-space data\n",
    "# create data loader for training set. It applies same to validation set as well\n",
    "train_dataset = MRIDataset(data_list['train'], acceleration=acc, center_fraction=cen_fract, use_seed=seed, cplx=cplx)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, num_workers=num_workers) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    <b>II. Network Architecture</b>\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the model: uncomment one of models below\n",
    "# model='CNN3L'\n",
    "# model='CNN4L'\n",
    "# model='UNet'\n",
    "model='DCGAN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2.1 Convolutional Neural Network (SRCNN)\n",
    "<h4> 2.1.1) 3 Layers-CNN\n",
    "<img src=\"SRCNN.png\"\n",
    "     alt=\"SRCNN figure\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model=='CNN3L':\n",
    "    class CNN_3L(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CNN_3L,self).__init__()\n",
    "            ### Patch Extraction n1=64 c=1 f1=9\n",
    "            self.block1=nn.Conv2d(1,64,kernel_size=9,padding=2)\n",
    "            ### Non-Linear Mapping n2=n1=64 f2=1\n",
    "            self.block2=nn.Conv2d(64,32,kernel_size=1,padding=2)\n",
    "            ### Image Reconstruction n3=n2=32 f3=5\n",
    "            self.block3=nn.Conv2d(32,1,kernel_size=5,padding=2)\n",
    "            self.RELU=nn.ReLU()\n",
    "        def forward(self,out):\n",
    "            out=self.block1(out)\n",
    "            out=self.RELU(out)\n",
    "            out=self.block2(out)\n",
    "            out=self.RELU(out)\n",
    "            out=self.block3(out)\n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2.1.2) 4 Layers-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model=='CNN4L':\n",
    "    class CNN_4L(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CNN_4L,self).__init__()\n",
    "            ### Patch Extraction n1=64 c=1 f1=9\n",
    "            self.block1=nn.Conv2d(1,64,kernel_size=9,padding=2)\n",
    "            ### Non-Linear Mapping n2=n1=64 f2=1\n",
    "            self.block2=nn.Conv2d(64,64,kernel_size=1,padding=2)\n",
    "            ### Image Reconstruction n3=n2=32 f3=5\n",
    "            self.block3=nn.Conv2d(64,32,kernel_size=5,padding=2)\n",
    "            ### Image Reconstruction n4=n=32 f3=3\n",
    "            self.block4=nn.Conv2d(32,1,kernel_size=3,padding=1)\n",
    "            self.RELU=nn.ReLU()\n",
    "        def forward(self,out):\n",
    "            out=self.block1(out)\n",
    "            out=self.RELU(out)\n",
    "            out=self.block2(out)\n",
    "            out=self.RELU(out)\n",
    "            out=self.block3(out)\n",
    "            out=self.RELU(out)\n",
    "            out=self.block4(out)\n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.2 UNet\n",
    "<img src=\"UNet.png\"\n",
    " alt=\"SRCNN figure\"\n",
    " style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-9d27f1741682>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'UNet'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mUNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbilinear\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "if model=='UNet':\n",
    "    class UNet(nn.Module):\n",
    "        def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "            super(UNet, self).__init__()\n",
    "            self.n_channels = n_channels\n",
    "            self.n_classes = n_classes\n",
    "            self.bilinear = bilinear\n",
    "            self.inc = DoubleConv(n_channels, 64)\n",
    "            self.down1 = Down(64, 128)\n",
    "            self.down2 = Down(128, 256)\n",
    "            self.down3 = Down(256, 512)\n",
    "            self.down4 = Down(512, 512)\n",
    "            self.up1 = Up(1024, 256, bilinear)\n",
    "            self.up2 = Up(512, 128, bilinear)\n",
    "            self.up3 = Up(256, 64, bilinear)\n",
    "            self.up4 = Up(128, 64, bilinear)\n",
    "            self.outc = OutConv(64, n_classes)\n",
    "        def forward(self, x):\n",
    "            x1 = self.inc(x)\n",
    "            x2 = self.down1(x1)\n",
    "            x3 = self.down2(x2)\n",
    "            x4 = self.down3(x3)\n",
    "            x5 = self.down4(x4)\n",
    "            x = self.up1(x5, x4)\n",
    "            x = self.up2(x, x3)\n",
    "            x = self.up3(x, x2)\n",
    "            x = self.up4(x, x1)\n",
    "            logits = self.outc(x)\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.3 DCGAN (Deep Convolution Generative Adversarial Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 2.3.1) Generator Network (UNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model=='DCGAN':\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "            super(Generator, self).__init__()\n",
    "            self.n_channels = n_channels\n",
    "            self.n_classes = n_classes\n",
    "            self.bilinear = bilinear\n",
    "            self.inc = DoubleConv(n_channels, 64)\n",
    "            self.down1 = Down(64, 128)\n",
    "            self.down2 = Down(128, 256)\n",
    "            self.down3 = Down(256, 512)\n",
    "            self.down4 = Down(512, 512)\n",
    "            self.up1 = Up(1024, 256, bilinear)\n",
    "            self.up2 = Up(512, 128, bilinear)\n",
    "            self.up3 = Up(256, 64, bilinear)\n",
    "            self.up4 = Up(128, 64, bilinear) \n",
    "            self.outc = OutConv(64, n_classes)\n",
    "        def forward(self, x):\n",
    "            x1 = self.inc(x)\n",
    "            x2 = self.down1(x1)\n",
    "            x3 = self.down2(x2)\n",
    "            x4 = self.down3(x3)\n",
    "            x5 = self.down4(x4)\n",
    "            x = self.up1(x5, x4)\n",
    "            x = self.up2(x, x3)\n",
    "            x = self.up3(x, x2)\n",
    "            x = self.up4(x, x1)\n",
    "            logits = self.outc(x)\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2.3.2) Discriminator Network (5 Layers CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model=='DCGAN':\n",
    "    class Discriminator(nn.Module):\n",
    "        def __init__(self, ngpu):\n",
    "            super(Discriminator, self).__init__()\n",
    "            self.ngpu = ngpu\n",
    "            self.main = nn.Sequential(\n",
    "                # input is (nc) x 64 x 64\n",
    "                nn.Conv2d(nc, ndf, 4, 3, 1, bias=False),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "                # state size. (ndf) x 32 x 32\n",
    "                nn.Conv2d(ndf, ndf * 2, 4, 3, 1, bias=False),\n",
    "                nn.BatchNorm2d(ndf * 2),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                # state size. (ndf*2) x 16 x 16\n",
    "                nn.Conv2d(ndf * 2, ndf * 4, 4, 3, 1, bias=False),\n",
    "                nn.BatchNorm2d(ndf * 4),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                # state size. (ndf*4) x 8 x 8\n",
    "                nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 0, bias=False),\n",
    "                nn.BatchNorm2d(ndf * 8),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                # state size. (ndf*8) x 4 x 4\n",
    "                nn.Conv2d(ndf * 8, 1, 4, 2, 0, bias=False),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        def forward(self, input):\n",
    "            return self.main(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    <b>III. Network Training</b>\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.1 Training Parameters Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs\n",
    "num_epochs = 5\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.2 Resource Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'  # check whether a GPU is available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.3 Loss Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BCELoss function\n",
    "if model=='DCGAN':\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Establish convention for real and fake labels during training\n",
    "    real_label = 1\n",
    "    fake_label = 0\n",
    "\n",
    "    # Setup Adam optimizers for both G and D\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "else:\n",
    "# Initialize MSELoss function\n",
    "    criterion=nn.MSELoss('Mean')\n",
    "    optimizer=optim.SGD(model.parameters,lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.4 Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "        x_data,y_data=data                         # undersampled and ground truth images\n",
    "        \n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        # Format batch\n",
    "        real_cpu = y_data.to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,), real_label, device=device)\n",
    "        \n",
    "        # Forward pass real batch through D\n",
    "        output = netD(real_cpu).view(-1)\n",
    "        \n",
    "        '''\n",
    "        print('rcpu',real_cpu.shape)\n",
    "        print('bsize',b_size)\n",
    "        print('lab',label)\n",
    "        \n",
    "        print('Dout',netD(real_cpu).shape)\n",
    "        print('Doutv',output.shape)\n",
    "        '''\n",
    "        \n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        #noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        noise = x_data.to(device)\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        #print((fake).shape)\n",
    "        #break\n",
    "        # Classify all fake batch with D\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Add the gradients from the all-real and all-fake batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "        \n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = netD(fake).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        \n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(train_loader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "        \n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "        \n",
    "        '''\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(train_loader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "        '''\n",
    "        \n",
    "        iters += 1\n",
    "        #print('ydat:',y_data.shape)\n",
    "        #print('fake:',fake.shape)\n",
    "        #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    <b>IV. Inference(Image Reconstruction)</b>\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='C:/Users/Ersalina/NeuralComp_G42/saved_params'\n",
    "model_path='C:/Users/Ersalina/NeuralComp_G42/saved_params/DCGAN_Gen01.pth'\n",
    "\n",
    "test_file_path='C:/Users/Ersalina/NC_Final_Assignment/fastMRI/NC2019MRI/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kspace2real(data):\n",
    "    return T.complex_abs(T.ifft2(T.to_tensor(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in sorted(os.listdir(test_file_path)):\n",
    "    with h5py.File(test_file_path+'/'+fname,  \"r\") as hf:\n",
    "        volume_kspace_4af = hf['kspace_4af'][()]\n",
    "        volume_kspace_8af = hf['kspace_8af'][()]\n",
    "        print(f'{fname} 4af 8af',volume_kspace_4af.shape, volume_kspace_8af.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(kspace2real(volume_kspace_4af).shape)\n",
    "inp=kspace2real(volume_kspace_4af)\n",
    "Shape=inp.shape\n",
    "inp=inp.view(Shape[0],1,Shape[1],Shape[2])\n",
    "# Reconstruct the Image\n",
    "#IR=model(inp[20])\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "plt.imshow(inp[20][0], cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "inp2=torch.randn(1,1,2,2)\n",
    "out=model(inp)\n",
    "print(out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
